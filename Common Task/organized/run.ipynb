{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from df_utils import *\n",
    "from training_utils import *\n",
    "from kalshi_utils import *\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# I found that loading the data initially is not the most consistent, so I did not include it in the run.py file.\n",
    "# I have util functions that can be used to load the bulk of the data, but I will not include them here.\n",
    "# For downloading the bulk data initially, I would recommend using the load_Source_data() function in the utils.py file.\n",
    "# And using it in a python notebook for a more controlled/manual initial download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Update the data\n",
    "# This will update the data from the sources and include entries up until yesterday's date.\n",
    "# This will provide us with a data entry that can be used for prediction of the current day.\n",
    "num_retries = 10\n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_NOAA_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_OM_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_WRH_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_Solar_Soil_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_Air_Quality_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "print(\"Data updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (20,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.340936024983726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (22,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.99102020263672\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Get city info and iterate through the cities\n",
    "# The workflow will be as follows:\n",
    "# 1. Load the data for the city - done\n",
    "# 2. Load the models for the city - done\n",
    "# 3. Predict the next day's data entry - done\n",
    "# 4. Place a bet on Kalshi\n",
    "city_info = get_city_info()\n",
    "kalshi = KalshiAPI()\n",
    "\n",
    "for city in city_info.keys():\n",
    "    if city == \"Austin\" or city == \"Chicago\":\n",
    "        continue\n",
    "    # Get paths for the city data\n",
    "    noaa_path = city_info[city]['noaa']\n",
    "    om_path = city_info[city]['om']\n",
    "    wrh_path = city_info[city]['wrh']\n",
    "    aq_path = city_info[city]['aq']\n",
    "    solar_path = city_info[city]['ss']\n",
    "    attn_lstm_path = city_info[city]['attn_lstm']\n",
    "    lstm_path = city_info[city]['lstm']\n",
    "    xgb_path = city_info[city]['xgb']\n",
    "    scaler_features_path = city_info[city]['scaler']\n",
    "    kalshi_ticker = city_info[city]['kalshi']\n",
    "    history_path = city_info[city]['history']\n",
    "    \n",
    "    # Load the data for the city\n",
    "    # all_df is the main dataframe that contains all the data combined. I included subsets of the dataframes as well,\n",
    "    # in case I need them in the future. Predictor is simply the last row. I extracted it so that it does not get\n",
    "    # deleted by dropna.   \n",
    "    daily_df, daily_df_2, daily_df_3, all_df, predictor_final = load_all_dfs(noaa_path, om_path, solar_path, wrh_path, aq_path)\n",
    "    \n",
    "    # Load the models for the city\n",
    "    attn_lstm_model = load_model(attn_lstm_path)\n",
    "    lstm_model = load_model(lstm_path)\n",
    "    xgb_model = load_xgb(xgb_path)\n",
    "    scaler_features = load_scaler(scaler_features_path)\n",
    "    \n",
    "    # Prepare the data for prediction (using prep_data for ease sequence creation)\n",
    "    columns_to_ignore = ['date', 'next_day_max_temp']\n",
    "    target_column = 'next_day_max_temp'\n",
    "    data = all_df\n",
    "    features = data.drop(columns=columns_to_ignore).columns\n",
    "    target = target_column\n",
    "    # Split the data into training, validation, and testing sets\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    val_size = int(len(data) * 0.15)\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size+val_size]\n",
    "    test_data = data[train_size+val_size:]\n",
    "    # Create sequences\n",
    "    def create_sequences(features, target, seq_length):\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(features) - seq_length):\n",
    "            X.append(features[i:i+seq_length])\n",
    "            y.append(target[i+seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    # Create sequences for the training, validation, and testing sets\n",
    "    seq_length = 20  # Number of previous days to use as input\n",
    "    test_features = scaler_features.transform(test_data[features])\n",
    "    X_test, y_test = create_sequences(test_features, test_data[target].values, seq_length)\n",
    "    # Get the last sequence for prediction\n",
    "    last_sequence = X_test[-1]\n",
    "    last_result = y_test[-1]\n",
    "    # turn the last sequence into a tensor\n",
    "    last_sequence = torch.FloatTensor(last_sequence).unsqueeze(0)\n",
    "    \n",
    "    # Update the last sequence, by popping the first element and appending the new prediction\n",
    "    last_sequence = last_sequence.squeeze(0).tolist()\n",
    "    last_sequence.pop(0)\n",
    "    predictor_feature = predictor_final.drop(labels=columns_to_ignore)\n",
    "    predictor_feature = predictor_feature.fillna(0)\n",
    "    # reshape to (1, -1) for the scaler\n",
    "    predictor_feature = predictor_feature.values.reshape(1, -1)\n",
    "    predictor_feature = scaler_features.transform(predictor_feature).tolist()[0]\n",
    "    last_sequence.append(predictor_feature)\n",
    "    last_sequence = torch.FloatTensor([last_sequence])\n",
    "    \n",
    "    # predict using the models\n",
    "    attn_lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        attn_lstm_output = attn_lstm_model(last_sequence)\n",
    "    attn_lstm_output = attn_lstm_output.item()\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_output = lstm_model(last_sequence)\n",
    "    lstm_output = lstm_output.item()\n",
    "    # predict with xgb\n",
    "    xgb_output = xgb_model.predict(np.array(predictor_feature).reshape(1, -1))\n",
    "    xgb_output = xgb_output[0]\n",
    "    \n",
    "    # mean of the predictions\n",
    "    prediction = (attn_lstm_output + lstm_output + xgb_output) / 3\n",
    "    print(prediction)\n",
    "    # Proceed to Kalshi.\n",
    "    # 1 update old settlements for bookkeeping\n",
    "    kalshi.update_old_orders(history_path)\n",
    "    \n",
    "    # 2. check market status\n",
    "    exchange_status = kalshi.get_exchange_status()\n",
    "    if exchange_status.trading_active:\n",
    "        # get event\n",
    "        event = kalshi.format_event_ticker(kalshi_ticker)\n",
    "        # get markets\n",
    "        markets = kalshi.get_event_markets(event)\n",
    "        # place bets\n",
    "        order_list = kalshi.place_orders(prediction, markets)\n",
    "        # log the orders\n",
    "        kalshi.log_order(order_list, history_path, prediction)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
