{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models.utils import *\n",
    "from models.training_utils import *\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# I found that loading the data initially is not the most consistent, so I did not include it in the run.py file.\n",
    "# I have util functions that can be used to load the bulk of the data, but I will not include them here.\n",
    "# For downloading the bulk data initially, I would recommend using the load_Source_data() function in the utils.py file.\n",
    "# And using it in a python notebook for a more controlled/manual initial download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'training_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/weights/Austin_attn_lstm.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\Spring 2024\\CS542\\Common Task\\organized\\models\\training_utils.py:354\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[1;32m--> 354\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\venv\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\venv\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\venv\\lib\\site-packages\\torch\\serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'training_utils'"
     ]
    }
   ],
   "source": [
    "load_model('./models/weights/Austin_attn_lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Update the data\n",
    "# This will update the data from the sources and include entries up until yesterday's date.\n",
    "# This will provide us with a data entry that can be used for prediction of the current day.\n",
    "num_retries = 10\n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_NOAA_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_OM_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_WRH_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_Solar_Soil_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "for i in range(num_retries):\n",
    "    try:\n",
    "        #update_Air_Quality_data()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Retrying... {i+1}/{num_retries}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "print(\"Data updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (13,28) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'training_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m daily_df, daily_df_2, daily_df_3, all_df, predictor_final \u001b[38;5;241m=\u001b[39m load_all_dfs(noaa_path, om_path, solar_path, wrh_path, aq_path)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Load the models for the city\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m attn_lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_lstm_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m load_model(lstm_path)\n\u001b[0;32m     33\u001b[0m scaler_features \u001b[38;5;241m=\u001b[39m load_scaler(scaler_features_path)\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\Spring 2024\\CS542\\Common Task\\organized\\models\\training_utils.py:354\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[1;32m--> 354\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\venv\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\venv\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kmfoc\\My Drive\\BU Files\\BU Masters\\Work\\venv\\lib\\site-packages\\torch\\serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'training_utils'"
     ]
    }
   ],
   "source": [
    "# Step 2: Get city info and iterate through the cities\n",
    "# The workflow will be as follows:\n",
    "# 1. Load the data for the city - done\n",
    "# 2. Load the models for the city - done\n",
    "# 3. Look at the new data entry\n",
    "# 4. Predict the new data entry, update model weights, and save the model weights\n",
    "# 5. Predict the next day's data entry\n",
    "# 6. Place a bet on Kalshi\n",
    "city_info = get_city_info()\n",
    "\n",
    "for city in city_info.keys():\n",
    "    if city != 'Austin':\n",
    "        continue\n",
    "    # Get paths for the city data\n",
    "    noaa_path = city_info[city]['noaa']\n",
    "    om_path = city_info[city]['om']\n",
    "    wrh_path = city_info[city]['wrh']\n",
    "    aq_path = city_info[city]['aq']\n",
    "    solar_path = city_info[city]['ss']\n",
    "    attn_lstm_path = city_info[city]['attn_lstm']\n",
    "    lstm_path = city_info[city]['lstm']\n",
    "    scaler_features_path = city_info[city]['scaler']\n",
    "    \n",
    "    # Load the data for the city\n",
    "    # all_df is the main dataframe that contains all the data combined. I included subsets of the dataframes as well,\n",
    "    # in case I need them in the future. Predictor is simply the last row. I extracted it so that it does not get\n",
    "    # deleted by dropna.   \n",
    "    daily_df, daily_df_2, daily_df_3, all_df, predictor_final = load_all_dfs(noaa_path, om_path, solar_path, wrh_path, aq_path)\n",
    "    \n",
    "    # Load the models for the city\n",
    "    attn_lstm_model = load_model(attn_lstm_path)\n",
    "    lstm_model = load_model(lstm_path)\n",
    "    scaler_features = load_scaler(scaler_features_path)\n",
    "    \n",
    "    # Prepare the data for prediction (using prep_data for ease sequence creation)\n",
    "    columns_to_ignore = ['date', 'next_day_max_temp']\n",
    "    target_column = 'next_day_max_temp'\n",
    "    data = all_df\n",
    "    features = data.drop(columns=columns_to_ignore).columns\n",
    "    target = target_column\n",
    "    # Split the data into training, validation, and testing sets\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    val_size = int(len(data) * 0.15)\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size+val_size]\n",
    "    test_data = data[train_size+val_size:]\n",
    "    # Create sequences\n",
    "    def create_sequences(features, target, seq_length):\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(features) - seq_length):\n",
    "            X.append(features[i:i+seq_length])\n",
    "            y.append(target[i+seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    # Create sequences for the training, validation, and testing sets\n",
    "    seq_length = 20  # Number of previous days to use as input\n",
    "    test_features = scaler_features.transform(test_data[features])\n",
    "    X_test, y_test = create_sequences(test_features, test_data[target].values, seq_length)\n",
    "    # Get the last sequence for prediction\n",
    "    last_sequence = X_test[-1]\n",
    "    last_result = y_test[-1]\n",
    "    # turn the last sequence into a tensor\n",
    "    last_sequence = torch.FloatTensor(last_sequence).unsqueeze(0)\n",
    "    \n",
    "    # Predict the new data entry\n",
    "    # Predict using the attention LSTM model, then update its weights\n",
    "    attn_lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        attn_lstm_output = attn_lstm_model(last_sequence)\n",
    "    attn_lstm_output = attn_lstm_output.item()\n",
    "\n",
    "    # Predict using the LSTM model, then update its weights\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_output = lstm_model(last_sequence)\n",
    "    lstm_output = lstm_output.item()\n",
    "    \n",
    "    # Update the last sequence, by popping the first element and appending the new prediction\n",
    "    last_sequence = last_sequence.squeeze(0).tolist()\n",
    "    last_sequence.pop(0)\n",
    "    predictor_final = predictor_final.to_numpy().tolist()\n",
    "    last_sequence.append(predictor_final)\n",
    "    last_sequence = torch.FloatTensor([last_sequence])\n",
    "    \n",
    "    # predict using the updated models\n",
    "    attn_lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        attn_lstm_output = attn_lstm_model(last_sequence)\n",
    "    attn_lstm_output = attn_lstm_output.item()\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        lstm_output = lstm_model(last_sequence)\n",
    "    lstm_output = lstm_output.item()\n",
    "    \n",
    "    # mean of the two predictions\n",
    "    prediction = (attn_lstm_output + lstm_output) / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
